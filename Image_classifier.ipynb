{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image_classifier.ipynb","provenance":[{"file_id":"1xLD-_sFOl15FY0Rnw01DN1IKf0SfbmT6","timestamp":1576169932117}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"84nSiUZsnGDn","colab_type":"code","colab":{}},"source":["# Connect Google - Drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","!ln -s \"/content/gdrive/My Drive\" \"/content/mydrive\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Uyg1CU2ORSX","colab_type":"code","colab":{}},"source":["from google.colab import files"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UadkD8sJeFyd","colab_type":"code","colab":{}},"source":["# Downloading the image dataset from Dropbox with !wget\n","\n","import os\n","os.chdir(\"/content\")\n","\n","!wget https://www.dropbox.com/s/sj686ss02d3mqlx/image_classfier.tar.gz\n","\n","# Uncompress zipped file\n","!tar -xzvf image_classfier.tar.gz -C /content"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0drtC6lfsGLF","colab_type":"code","colab":{}},"source":["# Tensorboard\n","\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip -o ngrok-stable-linux-amd64.zip\n","\n","LOG_DIR = '/content/gdrive/My Drive/image_classifier/log'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 8080 &'\n","    .format(LOG_DIR)\n",")\n","\n","get_ipython().system_raw('./ngrok http 8080 &')\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-mywdlE8ZyV","colab_type":"code","colab":{}},"source":["# Load model/confusion matrix\n","\n","# Importing the required libraries\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from keras.models import load_model\n","import pandas as pd\n","from numpy import argmax\n","\n","# Confusion matrix\n","\n","def plot_confusion_matrix(cm, target_names,title='Confusion matrix',cmap=None, normalize=False):\n","  accuracy = np.trace(cm) / float(np.sum(cm))\n","  misclass = 1 - accuracy\n","  if cmap is None:\n","      cmap = plt.get_cmap('Blues')\n","  plt.figure(figsize=(10, 8))\n","  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","  plt.title(title)\n","  plt.colorbar()\n","\n","  if target_names is not None:\n","      tick_marks = np.arange(len(target_names))\n","      plt.xticks(tick_marks, target_names, rotation=45)\n","      plt.yticks(tick_marks, target_names)\n","\n","  if normalize:\n","      cm = cm.astype('float32') / cm.sum(axis=1)\n","      cm = np.round(cm,2)\n","        \n","\n","  thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","      if normalize:\n","          plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n","                   horizontalalignment=\"center\",\n","                   color=\"white\" if cm[i, j] > thresh else \"black\")\n","      else:\n","          plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                   horizontalalignment=\"center\",\n","                   color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","  plt.tight_layout()\n","  plt.ylabel('True label')\n","  plt.xlabel(\"Predicted label\\naccuracy={:0.4f}\\n misclass={:0.4f}\".format(accuracy, misclass))\n","  plt.savefig('/content/mydrive/image_classifier/cm.jpg') \n","  plt.show() \n","\n","filepath = '/content/mydrive/image_classifier/weights/weights_vgg16_scratch2.h5'\n","\n","\n","model_trained = load_model(filepath)\n","\n","y_pred = model_trained.predict(X_test) # predicted label\n","y_pred = np.argmax(y_pred, axis=1)\n","\n","y_true = np.argmax(y_test, axis=1) # true label\n","\n","confusion_mtx = confusion_matrix(y_true, y_pred)\n","\n","#----------------------------------------------------------------------------------------------\n","\n","# plot the confusion matrix\n","\n","plot_confusion_matrix(confusion_mtx, normalize=False, target_names=pd.unique(labelList))\n","print('')\n","\n","#-----------------------------------------------------------------------------------------------\n","\n","# plot classifcation report\n","\n","print(classification_report(y_true, y_pred, target_names=pd.unique(labelList)))\n","print('')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFrN7g-lr5MZ","colab_type":"code","colab":{}},"source":["# Testing on random images\n","\n","# Importing the required libraries\n","\n","from numpy import argmax\n","import matplotlib.pyplot as plt\n","from keras.preprocessing.image import load_img\n","import pandas as pd\n","from keras.models import load_model\n","\n","# Random input image prediction\n","\n","target_names = pd.unique(labelList)\n","filepath = '/content/mydrive/image_classifier/weights/weights_vgg16_scratch2.h5'\n","model_trained = load_model(filepath)\n","\n","def predict_random(path):   \n","    # image pre_processing \n","    img_t = load_img(path)\n","    # show image\n","    plt.figure(figsize=(5,5))\n","    plt.imshow(img_t)\n","    plt.axis('off')\n","    plt.show()\n","    \n","    img_t = img_t.resize((imgSize,imgSize))\n","    img_t = np.array(img_t)\n","    img_t = img_t/255.0\n","    \n","    probs = model_trained.predict(np.expand_dims(img_t, axis=0))\n","    \n","    for idx in probs.argsort()[0][::-1][:5]:\n","      print(\"{:.2f}%\".format(probs[0][idx]*100), \"\\t\", target_names[idx].split(\"-\")[-1])\n","\n","\n","# Random images\n","\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_car.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_car2.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_cross2.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_cross3.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_human.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_person2.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_person3.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_person4.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_person5.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_person6.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_tram.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_tram2.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_tram3.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_car3.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_tram4.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_zebra.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_zebra2.jpg')\n","\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_person7.jpg')\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_tram5.jpg')\n","\n","predict_random('/content/gdrive/My Drive/image_classifier/random/rn_person8.jpg')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYaddVhDtxft","colab_type":"code","colab":{}},"source":["# Image calssifier\n","\n","# Importing the required libraries\n","\n","import os,shutil,math,scipy,cv2\n","from tqdm import tqdm\n","import cv2\n","from skimage.io import imread\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils.np_utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from numpy import argmax\n","from keras.preprocessing.image import ImageDataGenerator\n","import matplotlib.pyplot as plt\n","import random as rn\n","from keras.models import Sequential\n","from keras.layers.convolutional import Conv2D, MaxPooling2D \n","from keras.layers import Flatten, Dropout, Dense, GlobalAveragePooling2D, BatchNormalization, Activation\n","from keras.optimizers import Adam, RMSprop, SGD\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n","import itertools\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg19 import VGG19\n","from keras.applications.resnet import ResNet50\n","from keras.preprocessing.image import load_img\n","import pandas as pd\n","import tensorflow as tf\n","\n","# ------------------------------------------------------------------------------------\n","\n","# Showing model final history\n","\n","def finalHistory(history):\n","\t\n","  fig, axis = plt.subplots(1,2, figsize=(15,5))\n","  accuracy = history.history['acc']\n","  val_accuracy = history.history['val_acc']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","  epochs = history.epoch\n","\t \n","  axis[0].set_title('Training and validation loss')\n","  axis[0].plot(epochs, loss, 'r', label = 'Training loss')\n","  axis[0].plot(epochs, val_loss, 'b', label = 'Validation loss', linestyle='--')\n","  axis[0].set_xlabel('epochs')\n","  axis[0].set_ylabel('loss')\n","  \n","  axis[1].set_title('Training and validation accuracy')\n","  axis[1].plot(epochs, accuracy, 'r', label = 'Training accuracy')\n","  axis[1].plot(epochs, val_accuracy, 'b', label = 'Validation accuracy', linestyle='--')\n","  axis[1].set_xlabel('epochs')\n","  axis[1].set_ylabel('accuracy')\n","  \n","  axis[0].legend()\n","  axis[1].legend()\n","\n","  plt.savefig('/content/mydrive/image_classifier/saved/history_feature_e_256_5.png')\n","\n","#-----------------------------------------------------------------------------------\n","\n","# Read in the data\n","\n","imgSize = 150\n","batchSize = 128\n","\n","imgList = [] # empty image list\n","labelList = [] # empty label list\n","\n","def label_assignment(img, label):\n","\treturn label\n","\n","def trainingData(label, objectDirs): # training data function, input the label and te object dictionaries\n","\t\n","  for img in tqdm(os.listdir(objectDirs)): # using tqdm for listing the images in the dctionaries\n","    \n","    path = os.path.join(objectDirs, img) # join the dictionary with the right image\n","    img = cv2.imread(path,cv2.IMREAD_COLOR) # read in the path in rgb colours\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img = cv2.resize(img,(imgSize,imgSize)) # resize the images for a specified size (150,150)  \n","    label = label_assignment(img, label)\n","    imgList.append(np.array(img)) # transform images into numpy array and add them to the empty image_list\n","    labelList.append(str(label)) # transform the labels into string and add them to the empty label_list\n","    \n","#---------------------------------------------------------------------------------------\n","\n","# Dataset libraries\n","\n","carDir = '/content/image_classfier/car'\n","crossing_signDir = '/content/image_classfier/crossing_sign'\n","personDir = '/content/image_classfier/person'\n","tramDir = '/content/image_classfier/tram'\n","zebraDir = '/content/image_classfier/zebra'\n","\n","weights_path = '/content/mydrive/image_classifier/transfer_weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n","\n","trainingData('car', carDir)\n","trainingData('crossing sign', crossing_signDir)\n","trainingData('person', personDir)\n","trainingData('tram', tramDir)\n","trainingData('zebra', zebraDir)\n","\n","#---------------------------------------------------------------------------------------\n","\n","# Label encoding\n","\n","numClasses = len(np.unique(labelList))\n","\n","le = LabelEncoder() \n","\n","e_labelList = le.fit_transform(labelList) # transform the string labels to numeric values. It is a necessary step for the model\n","e_labelList = to_categorical(e_labelList, numClasses) # split the labels into 4 categories -- ONE HOT ENCODING\n","imgList = np.array(imgList)\n","imgList = imgList/255.0 # normalization [0,1] converting the images into numpy arrays\n","\n","#-------------------------------------------------------------------------------------\n","\n","# Train Validation split\n","\n","X_train, X_valid, y_train, y_valid = train_test_split(imgList, e_labelList, test_size=0.20, random_state=42, shuffle=True)\n","\n","# Train Test split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=42,  shuffle=True)\n","\n","#-----------------------------------------------------------------------------------\n","\n","# A little data augmentation\n","\n","aug_gen = ImageDataGenerator( \n","        width_shift_range=0.1,  \n","        height_shift_range=0.1) \n","\n","aug_gen.fit(X_train)\n","\n","# Showing the shape after splitting the dataset\n","\n","print('')\n","print('Shape of object classes:', numClasses) # number of object classes\n","print('')\n","print('Shape of training images:', X_train.shape) # number of training samples\n","print('Shape of training labels:', y_train.shape) # number of training samples\n","print('Shape of validation images:', X_valid.shape) # number of testing samples\n","print('Shape of validation labels:', y_valid.shape) # number of testing labels\n","print('Shape of testing images:', X_test.shape) # number of testing samples\n","print('Shape of testing labels:', y_test.shape) # number of testing labels\n","\n","#-------------------------------------------------------------------------------------\n","\n","# Plot some random images\n","\n","fig, axis = plt.subplots(5,2)\n","fig.set_size_inches(15,15)\n","for i in range(5):\n","  for j in range(2):\n","    randLabel = rn.randint(0, len(labelList))\n","    axis[i, j].imshow(imgList[randLabel])\n","    axis[i, j].set_title('Object: '+ labelList[randLabel])\n","    axis[i, j].grid(color='w', linestyle='-', linewidth=0.5)\n","\n","plt.tight_layout()\n","plt.savefig('/content/mydrive/image_classifier/saved')\n","plt.show()\n","\n","#-------------------------------------------------------------------------------------\n","\n","# Transfer learning model (Feature extractor model)\n","\n","base_model = VGG16(include_top=False, input_shape = (imgSize,imgSize,3), weights = None, pooling='avg')\n","\n","base_model.load_weights(weights_path)\n","\n","for i in range (len(base_model.layers)):\n","    print (i,base_model.layers[i])\n","  \n","for layer in base_model.layers[15:]:\n","    layer.trainable=True\n","for layer in base_model.layers[0:15]:\n","    layer.trainable=False\n","\n","model = Sequential()\n","model.add(base_model)\n","model.add(GlobalAveragePooling2D())\n","model.add(Dropout(0.5))\n","model.add(Dense(5,activation='softmax'))\n","model.summary()\n","\n","#--------------------------------------------------------------------------------------\n","\n","# Transfer learning model (1 layer unforzen - Fine - tuning model)\n","\n","base_model = VGG16(include_top=False, input_shape = (imgSize,imgSize,3), weights = None, pooling='avg')\n","\n","base_model.load_weights(weights_path)\n","\n","for layer in base_model.layers:\n","    layer.trainable = False\n","    \n","for layer in base_model.layers:\n","    print(layer,layer.trainable)\n","\n","model = Sequential()\n","model.add(base_model)\n","model.add(GlobalAveragePooling2D())\n","model.add(Dropout(0.5))\n","model.add(Dense(5,activation='softmax'))\n","model.summary()\n","\n","#-------------------------------------------------------------------------------------\n","\n","# Scratch model\n","\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), input_shape=(imgSize, imgSize, 3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Conv2D(32, (3, 3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","model.add(Conv2D(64, (3, 3)))\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.3))\n","\n","model.add(Flatten())\n","model.add(Dense(64))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(numClasses))\n","model.add(Activation('softmax'))\n","model.summary()\n","\n","from keras.utils import plot_model\n","\n","plot_model(model, to_file='/content/mydrive/image_classifier/saved/convolutional_neural_network.png')\n","\n","#---------------------------------------------------------------------------------------\n","\n","# Compile the model\n","\n","model.compile(loss = 'categorical_crossentropy', optimizer=Adam(lr=1e-4) , metrics = ['accuracy'])\n","\n","#--------------------------------------------------------------------------------------------\n","\n","# Model callbacks\n","\n","filepath = '/content/mydrive/image_classifier/weights/weights_vgg16_scratch2.h5'\n","checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode ='max') # save only the best model\n","earlystop = EarlyStopping(monitor = 'val_loss', patience = 5, verbose = 1, mode ='min') # early model stop, which gives the best validation loss #  min_delta=0.001\n","\n","checkpoint = ModelCheckpoint(\n","    filepath,\n","    monitor='val_loss',\n","    verbose=1,\n","    save_best_only=True,\n","    mode='min',\n","    save_weights_only=False,\n","    period=1\n",")\n","earlystop = EarlyStopping( # if the valdaiton loss not decreasing after  10 steps, to prevent over fitting we stop the trainign process\n","    monitor='val_loss',\n","    min_delta=0.001,\n","    patience=10,\n","    verbose=1,\n","    mode='auto'\n",")\n","\n","callbackList = [checkpoint, earlystop]\n","\n","#--------------------------------------------------------------------------------------------\n","\n","num_train = X_train.shape[0]\n","num_test = X_test.shape[0]\n","num_valid = X_valid.shape[0]\n","\n","# Model training\n","\n","history = model.fit_generator(\n","    aug_gen.flow(X_train,y_train, batch_size=batchSize),\n","    validation_data  = (X_valid,y_valid),\n","    validation_steps = num_valid//batchSize,\n","    steps_per_epoch  = num_train//batchSize,\n","    epochs = 150, \n","    verbose = 1,\n","    callbacks=callbackList)\n","\n","print('Training complete')\n","\n","#--------------------------------------------------------------------------------------------\n","\n","# Model evaluate\n","\n","finalHistory(history)\n","\n","score = model.evaluate(X_test, y_test)\n","\n","print('Model test loss:', score[0])\n","print('Model test accuracy:', score[1])\n"],"execution_count":0,"outputs":[]}]}